# Research Templates - Dokumentacja

**Wersja:** 1.0
**Data:** 2025-12-29
**Podstawa:** PROPOZYCJA-1-Research-Branch-Templates.md

---

## Spis treÅ›ci

1. [Wprowadzenie](#wprowadzenie)
2. [Kiedy uÅ¼ywaÄ‡ Research Templates](#kiedy-uÅ¼ywaÄ‡-research-templates)
3. [Katalog szablonÃ³w](#katalog-szablonÃ³w)
4. [Workflows i przepÅ‚ywy](#workflows-i-przepÅ‚ywy)
5. [PrzykÅ‚ady uÅ¼ycia](#przykÅ‚ady-uÅ¼ycia)
6. [Best Practices](#best-practices)
7. [FAQ](#faq)

---

## Wprowadzenie

### Czym sÄ… Research Templates?

Research Templates to zestaw **7 szablonÃ³w dokumentacyjnych** zaprojektowanych do systematycznego prowadzenia badaÅ„, eksperymentÃ³w i eksploracji konceptÃ³w w projektach.

**Problem, ktÃ³ry rozwiÄ…zujÄ…:**
- âŒ Badania sÄ… ad-hoc, wyniki ginÄ… w Slack/email
- âŒ Eksperymenty nie sÄ… powtarzalne (brak dokumentacji metodologii)
- âŒ Decyzje oparte na "gut feeling" zamiast danych
- âŒ Wiedza z PoC/spike'Ã³w przepada po zakoÅ„czeniu

**RozwiÄ…zanie:**
- âœ… Formalizacja procesu badawczego (Hypothesis â†’ Experiment â†’ Findings)
- âœ… Audytowalny trail eksperymentÃ³w (compliance, knowledge retention)
- âœ… Data-driven decisions (ADR wspierane empirycznymi danymi)
- âœ… Systematyczna eksploracja alternatyw (scoring matrix, trade-offs)

### Dla kogo?

**ZespoÅ‚y software:**
- Spike solutions (Scrum/Agile)
- PoC (Proof of Concept) dla nowych technologii
- Performance optimization research
- Architecture exploration (A/B testing approaches)

**ZespoÅ‚y R&D:**
- Formalizacja hipotez naukowych
- Tracking eksperymentÃ³w (lab notebooks)
- Publikacja wynikÃ³w (papers, reports)

**ZespoÅ‚y produktowe:**
- UX research spikes
- Business model validation (pricing, features)
- Market research documentation

**BranÅ¼e regulowane:**
- Clinical trials (pharma, medical devices)
- Financial compliance (audit trails)
- Quality assurance (manufacturing)

---

## Kiedy uÅ¼ywaÄ‡ Research Templates

### Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Czy masz pytanie wymagajÄ…ce badaÅ„?      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   TAK           â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Czy potrzebujesz porÃ³wnaÄ‡ wiele opcji?       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚ TAK         â”‚ NIE
    â–¼             â–¼
ALTERNATIVE-  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
EXPLORATION   â”‚ Czy to krÃ³tkie badanie? â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
             â”‚ TAK (<5 dni)â”‚ NIE (>1 tydzieÅ„)
             â–¼             â–¼
        SPIKE-SOLUTION  HYPOTHESIS-DOC
                           â†“
                     EXPERIMENT-LOG
                           â†“
                        POC-DOC
```

### Quick Reference

| Szablon | Kiedy uÅ¼ywaÄ‡ | Czas trwania | Output |
|---------|--------------|--------------|--------|
| **HYPOTHESIS-DOC** | Masz hipotezÄ™ do zwalidowania | 4-8 tygodni | Validated/Invalidated hypothesis |
| **EXPERIMENT-LOG** | Prowadzisz eksperyment | 2-6 tygodni | Data, metrics, conclusions |
| **POC-DOC** | Potrzebujesz PoC techniczny | 2-4 tygodnie | Proceed/Pivot/Stop recommendation |
| **SPIKE-SOLUTION** | Szybkie pytanie techniczne/UX/biznesowe | 2-5 dni | YES/NO/CONDITIONAL answer |
| **RESEARCH-FINDINGS** | Agregacja wynikÃ³w z wielu badaÅ„ | N/A (raport) | Executive summary, recommendations |
| **ALTERNATIVE-EXPLORATION** | WybÃ³r miÄ™dzy 3+ opcjami | 2-4 tygodnie | Ranking, recommendation |
| **CONCEPT-BRANCH** | Parallel exploration dwÃ³ch podejÅ›Ä‡ | 4-8 tygodni | Merge/Kill decision |

---

## Katalog szablonÃ³w

### 1. HYPOTHESIS-DOC (Hypothesis Document)

**Grupa:** research
**Domena:** innovation
**Plik:** [HYPOTHESIS-DOC.md](./HYPOTHESIS-DOC.md)

**Opis:**
Formalizacja hipotezy badawczej z kryteriami walidacji. UÅ¼ywany przed rozpoczÄ™ciem eksperymentu lub PoC.

**Struktura kluczowa:**
- Stwierdzenie hipotezy (H0/H1)
- ZaÅ‚oÅ¼enia
- Kryteria sukcesu/poraÅ¼ki (measurable!)
- Metodologia walidacji
- Timeboxing

**Kiedy uÅ¼ywaÄ‡:**
- Przed rozpoczÄ™ciem kosztownego eksperymentu/PoC
- Gdy potrzebujesz alignment zespoÅ‚u na tym, CO badamy i JAK zmierzymy sukces
- Gdy wymagana jest formalna aprobata (przed alokacjÄ… budÅ¼etu)

**PrzykÅ‚ad:** [HYPOTHESIS-DOC-001-migracja-postgres-mongodb.md](../../examples/research/HYPOTHESIS-DOC-001-migracja-postgres-mongodb.md)

---

### 2. EXPERIMENT-LOG (Experiment Log)

**Grupa:** research
**Domena:** innovation
**Plik:** [EXPERIMENT-LOG.md](./EXPERIMENT-LOG.md)

**Opis:**
Tracking eksperymentu - setup, execution, observations (timestamped), results, analysis.

**Struktura kluczowa:**
- Link do hipotezy (HYPOTHESIS-DOC)
- Setup eksperymentu (environment, dataset, tools)
- Procedura wykonania (kroki, variables)
- **Obserwacje (timestamped!)** - chronologiczny log
- Dane i metryki
- Analiza wynikÃ³w (statistical significance)
- Wnioski (hipoteza potwierdzona/odrzucona)

**Kiedy uÅ¼ywaÄ‡:**
- Podczas wykonywania eksperymentu (live documentation)
- Gdy potrzebujesz audytowalnego trail (compliance)
- Gdy wyniki bÄ™dÄ… podstawÄ… decyzji (ADR)

**Best practice:**
- Zapisuj obserwacje w czasie rzeczywistym (timestamped entries)
- Dokumentuj takÅ¼e nieoczekiwane wyniki (surprises, anomalies)
- Statystyczna analiza wynikÃ³w (p-values, confidence intervals)

**PrzykÅ‚ad:** [EXPERIMENT-LOG-001-mongodb-benchmark.md](../../examples/research/EXPERIMENT-LOG-001-mongodb-benchmark.md)

---

### 3. POC-DOC (Proof of Concept)

**Grupa:** research
**Domena:** engineering
**Plik:** [POC-DOC.md](./POC-DOC.md)

**Opis:**
Dokumentacja Proof of Concept z validation criteria i decision framework (Proceed/Pivot/Stop).

**Struktura kluczowa:**
- Cel PoC (problem/opportunity)
- Zakres (In/Out)
- PodejÅ›cie techniczne (high-level architecture)
- **Success criteria** (functional, non-functional, business)
- Wyniki i metryki (vs criteria)
- Zidentyfikowane luki/ryzyka
- **Rekomendacja:** PROCEED / PIVOT / STOP (z uzasadnieniem)

**Kiedy uÅ¼ywaÄ‡:**
- Przed adopcjÄ… nowej technologii (evaluate feasibility)
- Przed duÅ¼Ä… inwestycjÄ… (de-risk decision)
- Gdy potrzebujesz empirycznych danych dla ADR

**Output:**
- Go/No-Go decision (data-driven)
- Lista warunkÃ³w kontynuacji (jeÅ›li PROCEED)
- Zidentyfikowane ryzyka i gaps

**PrzykÅ‚ad:** [POC-DOC-001-mongodb-pilot-deployment.md](../../examples/research/POC-DOC-001-mongodb-pilot-deployment.md)

---

### 4. SPIKE-SOLUTION (Spike Solution Template)

**Grupa:** research
**Domena:** multi (tech/ux/business)
**Plik:** [SPIKE-SOLUTION.md](./SPIKE-SOLUTION.md)

**Opis:**
Lightweight template dla timeboxed spike solutions (max 2-5 dni). Szybka odpowiedÅº na konkretne pytanie.

**Typy spike'Ã³w:**
- **Technical Spike:** API evaluation, library comparison, performance testing
- **UX Spike:** Usability testing, wireframing, accessibility audit
- **Business Spike:** Pricing model, market research, competitor analysis

**Struktura kluczowa:**
- Pytanie badawcze (konkretne, answerable)
- Dlaczego teraz (business value, blocker status)
- **Timebox:** Max 5 dni (HARD STOP!)
- PodejÅ›cie (methodology)
- Odkrycia (daily log)
- **OdpowiedÅº:** YES / NO / YES with conditions / NEEDS MORE RESEARCH

**Kiedy uÅ¼ywaÄ‡:**
- Scrum Sprint Planning (blocker przed User Story)
- Szybka walidacja zaÅ‚oÅ¼enia (2-3 dni wystarczÄ…)
- Risk reduction (przed heavy investment)

**Zasada spike:**
- **Timeboxed:** 2-5 dni, NO EXTENSIONS!
- **Focused:** Jedno pytanie, konkretna odpowiedÅº
- **Lightweight:** Prototype, nie production code
- **Actionable:** Output to decision (create ADR, update User Story, etc.)

**PrzykÅ‚ad:** [SPIKE-SOLUTION-001-rust-wasm-rendering.md](../../examples/research/SPIKE-SOLUTION-001-rust-wasm-rendering.md)

---

### 5. RESEARCH-FINDINGS (Research Findings Document)

**Grupa:** research
**Domena:** knowledge
**Plik:** [RESEARCH-FINDINGS.md](./RESEARCH-FINDINGS.md)

**Opis:**
Agregacja wynikÃ³w z wielu eksperymentÃ³w/PoC/spike'Ã³w. Executive summary dla stakeholderÃ³w.

**Struktura kluczowa:**
- Executive summary (TL;DR)
- Lista przeprowadzonych eksperymentÃ³w
- **Kluczowe odkrycia** (findings z evidence)
- Wzorce i trendy (cross-experiment insights)
- Niespodzianki i anomalie
- **Implikacje** (product, technical, business)
- Rekomendacje (priorytetyzowane)
- PrzyszÅ‚e badania (unanswered questions)

**Kiedy uÅ¼ywaÄ‡:**
- Po zakoÅ„czeniu research cycle (quarterly, project-based)
- Przed waÅ¼nÄ… decyzjÄ… strategicznÄ… (data dla Board/Execs)
- Jako podstawa dla roadmap planning

**Value:**
- **Knowledge aggregation:** Wyniki nie ginÄ… po zakoÅ„czeniu research
- **Pattern recognition:** Insights across experiments
- **Strategic alignment:** Research â†’ roadmap decisions

**PrzykÅ‚ad:** [RESEARCH-FINDINGS-001-performance-optimization-q1-2026.md](../../examples/research/RESEARCH-FINDINGS-001-performance-optimization-q1-2026.md)

---

### 6. ALTERNATIVE-EXPLORATION (Alternative Approach Analysis)

**Grupa:** research
**Domena:** decision-support
**Plik:** [ALTERNATIVE-EXPLORATION.md](./ALTERNATIVE-EXPLORATION.md)

**Opis:**
Systematyczna eksploracja i porÃ³wnanie alternatywnych podejÅ›Ä‡ do tego samego problemu (min 3 opcje).

**Struktura kluczowa:**
- Problem statement
- Constraints i evaluation criteria (weighted!)
- Zidentyfikowane alternatywy (min 3)
- **Detailed analysis** kaÅ¼dej opcji (pros/cons, cost, risks)
- **Scoring matrix** (quantitative comparison)
- PorÃ³wnanie (trade-offs visualization)
- **Rekomendacja** z uzasadnieniem
- Odrzucone opcje (why rejected, when reconsider)

**Kiedy uÅ¼ywaÄ‡:**
- WybÃ³r technologii (database, framework, cloud provider)
- Architectural decision (monolith vs microservices)
- Build vs Buy vs Partner decisions
- Vendor selection

**Best practice:**
- **Min 3 opcje** (forced exploration, nie binary choice)
- **Weighted criteria** (align stakeholders on priorities)
- **Quantitative scoring** (reduce bias, increase objectivity)
- **Document rejected options** (why + when to reconsider)

**PrzykÅ‚ad:** [ALTERNATIVE-EXPLORATION-001-baza-dokumentow.md](../../examples/research/ALTERNATIVE-EXPLORATION-001-baza-dokumentow.md)

---

### 7. CONCEPT-BRANCH (Concept Branch Document)

**Grupa:** research
**Domena:** innovation-management
**Plik:** [CONCEPT-BRANCH.md](./CONCEPT-BRANCH.md)

**Opis:**
Fork-merge framework dla rÃ³wnolegÅ‚ych gaÅ‚Ä™zi badawczych. Parallel exploration dwÃ³ch (lub wiÄ™cej) radykalnie rÃ³Å¼nych podejÅ›Ä‡.

**Struktura kluczowa:**
- Punkt rozwidlenia (fork point z parent concept)
- Dlaczego nowa gaÅ‚Ä…Åº (rationale)
- Alternatywne podejÅ›cie (key differences vs parent)
- **Progress tracking** (milestones, deliverables)
- **Learnings vs parent branch** (what works better/worse)
- **Merge/Kill/Continue decision** (final verdict)

**Kiedy uÅ¼ywaÄ‡:**
- Radykalnie rÃ³Å¼ne podejÅ›cia do tego samego problemu (np. REST vs GraphQL)
- High uncertainty - nie wiesz, ktÃ³re podejÅ›cie lepsze
- Parallel teams available (2+ teams, 4-8 weeks)
- Cost of wrong choice > cost of parallel exploration

**Pattern:**
```
Parent Concept (Problem defined)
    â”œâ”€ Branch A: Approach 1 (Team A, 4 weeks)
    â””â”€ Branch B: Approach 2 (Team B, 4 weeks)
         â†“
    Compare results â†’ Merge winner / Kill loser
```

**Value:**
- **Risk mitigation:** Validate assumptions before full commitment
- **Faster iteration:** Parallel vs sequential (4 weeks vs 8 weeks)
- **Team learning:** Both teams gain knowledge (retained even if branch killed)

**PrzykÅ‚ad:** [CONCEPT-BRANCH-001-realtime-collab-websocket.md](../../examples/research/CONCEPT-BRANCH-001-realtime-collab-websocket.md)

---

## Workflows i przepÅ‚ywy

### Workflow 1: Technology Selection (WybÃ³r technologii)

**Use case:** ZespÃ³Å‚ wybiera nowÄ… bazÄ™ danych dla moduÅ‚u analytics.

```
1. ALTERNATIVE-EXPLORATION
   â”œâ”€ Zidentyfikuj opcje (Elasticsearch, ClickHouse, TimescaleDB)
   â”œâ”€ Scoring matrix (performance, cost, team expertise)
   â””â”€ Shortlist: Top 2 opcje

2. HYPOTHESIS-DOC (dla Top 1)
   â”œâ”€ "ClickHouse oferuje lepsze performance/cost ratio"
   â””â”€ Success criteria defined

3. POC-DOC
   â”œâ”€ Implement prototype (2 weeks)
   â”œâ”€ Benchmark vs criteria
   â””â”€ Recommendation: PROCEED with conditions

4. ADR-XXX
   â””â”€ Final decision: Adopt ClickHouse (based on POC evidence)
```

**Czas:** 6-8 tygodni total
**Output:** Data-driven decision, audytowalny trail

---

### Workflow 2: Sprint Spike (Agile/Scrum)

**Use case:** Sprint Planning - User Story blocked przez technical unknown.

```
Sprint Planning
    â†“
User Story US-123 blocked
    â†“
SPIKE-SOLUTION (timeboxed 3 dni)
    â”œâ”€ Pytanie: "Can we use React Server Components?"
    â”œâ”€ Prototyping (Day 1-2)
    â”œâ”€ Testing (Day 3)
    â””â”€ Answer: YES, with conditions
         â†“
Update US-123
    â”œâ”€ Split into US-123a (RSC) + US-123b (refactor)
    â””â”€ Re-estimate (now we know!)
         â†“
Sprint continues (no longer blocked)
```

**Czas:** 3 dni (w ramach sprintu)
**Output:** Unblock User Story, clear implementation path

---

### Workflow 3: Research Cycle (Quarterly R&D)

**Use case:** Quarterly performance optimization research.

```
Q1 2026 Research Initiative
    â”œâ”€ HYPOTHESIS-001: MongoDB performance
    â”‚   â””â”€ EXPERIMENT-001: Benchmark (2 weeks)
    â”‚       â””â”€ POC-001: Pilot deployment (4 weeks)
    â”‚
    â”œâ”€ SPIKE-001: WASM Canvas rendering (3 days)
    â”‚
    â””â”€ RESEARCH-FINDINGS-001
        â”œâ”€ Aggregate all results
        â”œâ”€ Key findings: 70% performance improvement
        â”œâ”€ Recommendations: PROCEED with both initiatives
        â””â”€ Informs Q2 roadmap
             â†“
ADR-042: Migrate to MongoDB
ADR-045: Adopt Rust/WASM for Canvas
```

**Czas:** 14 tygodni research â†’ informuje Q2-Q3 roadmap
**Output:** Strategic decisions backed by empirical data

---

### Workflow 4: Parallel Exploration (Concept Branching)

**Use case:** Real-time collaboration - unknown best approach.

```
HYPOTHESIS-005: Real-time collab increases engagement
    â†“
Unknown: Which approach? (OT vs CRDT vs Locking)
    â†“
Parallel Branches (4 weeks)
    â”œâ”€ CONCEPT-BRANCH-001: WebSocket + Locking (Team A)
    â”‚   â”œâ”€ Prototype (2 weeks)
    â”‚   â”œâ”€ User testing (1 week)
    â”‚   â””â”€ Results: 90% comprehension, fast implementation
    â”‚
    â””â”€ CONCEPT-BRANCH-002: CRDT (Yjs) (Team B)
        â”œâ”€ Prototype (3 weeks)
        â”œâ”€ User testing (1 week)
        â””â”€ Results: 65% comprehension, complex implementation
             â†“
Compare branches
    â”œâ”€ WebSocket wins (simplicity, user clarity)
    â””â”€ Decision: MERGE Branch A, KILL Branch B
         â†“
ADR-048: Adopt WebSocket approach
Production implementation (8 weeks)
```

**Czas:** 4 weeks parallel + 8 weeks implementation = 12 weeks total
**Alternatywa sequential:** Try CRDT (5w) â†’ fail â†’ Try WebSocket (5w) = 10w + risk
**Value:** Higher confidence, mitigated risk, faster overall

---

## PrzykÅ‚ady uÅ¼ycia

### PrzykÅ‚ad 1: Database Migration (End-to-end)

**Scenariusz:** Ishkarim ma performance problems z PostgreSQL.

**Research workflow:**

1. **Problem identification** (Week 0)
   - User complaints: "Search is slow" (2.5s latency)
   - Business impact: 2 enterprise deals blocked

2. **ALTERNATIVE-EXPLORATION-001** (Week 1-2)
   - Options: MongoDB, Elasticsearch, PostgreSQL Optimized, Dgraph
   - Scoring matrix â†’ MongoDB wins (8.35/10)
   - Decision: Proceed with MongoDB validation

3. **HYPOTHESIS-001** (Week 3)
   - "MongoDB improves search 60%, graph queries 50%"
   - Success criteria defined, timeline 8 weeks

4. **EXPERIMENT-001** (Week 4-6)
   - Benchmark: 10K docs, synthetic workload
   - Results: Search -76% (2.5s â†’ 0.6s), Graph -61% (1.8s â†’ 0.7s)
   - Hypothesis: VALIDATED âœ…

5. **POC-001** (Week 7-10)
   - Pilot: 5K real docs, 20 users, 4 weeks production-like
   - Results: Real-world -68% search, -50% graph (slightly slower than benchmark, but excellent)
   - User satisfaction: NPS +85
   - Recommendation: **PROCEED** âœ…

6. **RESEARCH-FINDINGS-001** (Week 11)
   - Aggregate all results (EXP-001, POC-001, SPIKE-001 WASM)
   - Executive summary dla CTO/Board
   - Strategic implications: Unlock enterprise tier, Q2 roadmap

7. **ADR-042** (Week 12)
   - Decision: Migrate to MongoDB (based on research evidence)
   - Approved budget: $65K production migration

**Total:** 12 tygodni research â†’ High-confidence $65K investment decision

---

### PrzykÅ‚ad 2: Quick Spike (Agile Sprint)

**Scenariusz:** Sprint Planning - "Can we use WASM for Canvas rendering?"

**Spike workflow (3 dni):**

**Day 1:**
- Setup Rust + wasm-pack
- Minimal PoC (render rectangle)
- Verify compilation works âœ…

**Day 2:**
- Implement rendering loop (1000 nodes)
- Benchmark WASM vs JS
- Results: **71% faster!** ğŸ˜®

**Day 3:**
- Browser compatibility testing (97% coverage âœ…)
- Bundle size analysis (580KB gzipped - acceptable âœ…)
- Write SPIKE-SOLUTION-001 with answer: **YES, with conditions**

**Output:**
- User Story US-156 unblocked
- Clear implementation path (WASM + JS fallback)
- Confidence: HIGH (empirical data, not speculation)

**Time:** 3 dni (vs potentially 4 weeks wasted implementation bez spike)

---

## Best Practices

### 1. Timeboxing is Sacred

**Zasada:** KaÅ¼dy research initiative musi mieÄ‡ hard deadline.

- **Hypothesis:** 4-8 tygodni max
- **Experiment:** 2-6 tygodni max
- **PoC:** 2-4 tygodnie max
- **Spike:** 2-5 DNI max (no extensions!)

**Dlaczego:**
- Prevents analysis paralysis
- Forces prioritization (MVP scope)
- Ensures timely decision (opportunity cost)

**Enforcement:**
- Timebox w metadata (YAML frontmatter)
- Weekly check-ins (on track? extend/kill/pivot decision)
- Hard stop at deadline (output: decision or "inconclusive" + next steps)

---

### 2. Success Criteria Must Be Measurable

**Bad:** "System should be fast"
**Good:** "Search latency <1s (p95), 60% improvement vs baseline"

**Formula:**
```
Success Criteria = Metric + Target Value + Measurement Method
```

**Examples:**
- Performance: "API latency <100ms (p95), JMeter load test 100 concurrent users"
- User Satisfaction: "NPS >80, survey n=20 users"
- Cost: "Infrastructure <$1,000/month, AWS billing"
- Quality: "0 critical bugs, verified w test suite"

**Best practice:**
- Quantitative > Qualitative (when possible)
- Baseline + Target (pokazuje improvement %)
- Measurement method documented (reproducible)

---

### 3. Link Everything (Cross-References)

**Zasada:** Every research document linkuje do predecessors/successors.

```yaml
dependencies:
  - id: HYPOTHESIS-001
    type: requires
    reason: "Eksperyment waliduje tÄ™ hipotezÄ™"

impacts:
  - id: ADR-042
    type: informs
    reason: "Wyniki eksperymentu wspierajÄ… decyzjÄ™ w ADR"
```

**Value:**
- **Traceability:** Decision â†’ Research â†’ Evidence (audit trail)
- **Knowledge graph:** Visualize research dependencies
- **Context preservation:** Future readers understand "dlaczego"

**Tools:**
- Manual links (YAML cross-references w kaÅ¼dym dokumencie)
- Automated graph (parse YAML â†’ visualize w Cytoscape/Obsidian)

---

### 4. Document Failures (Not Just Successes)

**Zasada:** Rejected hypotheses sÄ… VALUABLE knowledge!

**Bad practice:**
- PoC fails â†’ document deleted â†’ repeat mistake later âŒ

**Good practice:**
- PoC fails â†’ POC-DOC with "STOP" recommendation + why âœ…
- Hypothesis rejected â†’ EXPERIMENT-LOG with invalidation reasons âœ…
- Spike answer "NO" â†’ SPIKE-SOLUTION documenting why NO âœ…

**Value:**
- **Prevent repetition:** Team 6 months later: "Let's try X!" â†’ "We tried, see EXP-042, failed because Y"
- **Organizational learning:** Failures teach more than successes
- **Justify decisions:** "We chose A over B because PoC-B failed on criteria C"

---

### 5. Involve Stakeholders Early

**Zasada:** Get buy-in BEFORE heavy research investment.

**Workflow:**
1. **Draft HYPOTHESIS-DOC** â†’ present to stakeholders
2. **Get approval** (budget, timeline, success criteria alignment)
3. **Run experiment/PoC**
4. **Present RESEARCH-FINDINGS** â†’ decision (ADR)

**Avoid:**
- Surprise stakeholders z results (they feel "decided without them")
- Ask for budget AFTER research complete (sunk cost fallacy)

**Best practice:**
- Weekly/bi-weekly research sync (short updates)
- "Preview" presentations (interim results, ask for feedback)
- Involve decision-makers w defining success criteria (ownership)

---

### 6. Keep It Lightweight (MVP Research)

**Zasada:** Research documents â‰  academic papers. Focus on decision-making, not perfection.

**Good:**
- Executive summary (1 paragraph TL;DR)
- Bullet points, tables, visuals
- Sufficient detail dla reproducibility
- Clear recommendation (Proceed/Pivot/Stop)

**Avoid:**
- 50-page reports (nobody reads)
- Perfection paralysis (80% good enough > 100% never done)
- Academic rigor dla sake of rigor (balance pragmatism vs thoroughness)

**Rule of thumb:**
- Hypothesis: 2-4 strony
- Experiment Log: 5-10 stron
- PoC: 8-12 stron
- Spike: 3-5 stron
- Research Findings: 10-15 stron
- Alternative Exploration: 10-15 stron

**If longer:** Consider splitting (too much scope).

---

### 7. Automate Where Possible

**Tooling recommendations:**

**Benchmarking:**
- JMeter, k6 (load testing, automated metrics collection)
- pytest-benchmark (Python performance tests)
- Datadog/Grafana (real-time monitoring, export data)

**Data collection:**
- Python scripts (automated data export, checksums verification)
- Git hooks (track experiment versions)
- CI/CD integration (regression testing dla performance)

**Visualization:**
- Jupyter Notebooks (interactive analysis, charts)
- Matplotlib/Plotly (Python visualization)
- Excel/Google Sheets (quick charts dla stakeholders)

**Templates:**
- Cookiecutter (generate research docs from templates)
- Scripts to pre-fill metadata (project, owner, dates)

---

## FAQ

### Q: Kiedy uÅ¼yÄ‡ SPIKE vs HYPOTHESIS+EXPERIMENT?

**A: Spike dla szybkich pytaÅ„ (2-5 dni), Hypothesis+Experiment dla dÅ‚ugich badaÅ„ (4-8 tygodni).**

**Spike:**
- Timeboxed (max 5 dni)
- Jedno konkretne pytanie
- Lightweight prototype
- Quick answer (YES/NO/CONDITIONAL)

**Hypothesis + Experiment:**
- DÅ‚uÅ¼szy cykl (4-8 tygodni)
- Comprehensive testing
- Statistical validation
- Detailed analysis

**Example:**
- "Can we use library X?" â†’ **SPIKE** (3 dni prototype)
- "Library X improves performance 50%?" â†’ **HYPOTHESIS+EXPERIMENT** (4 tygodnie benchmark, validation)

---

### Q: Czy muszÄ™ uÅ¼yÄ‡ wszystkich 7 szablonÃ³w dla jednego research initiative?

**A: NIE. UÅ¼ywaj tylko tych, ktÃ³re dodajÄ… wartoÅ›Ä‡.**

**Minimal research:**
- SPIKE-SOLUTION (3 dni) â†’ ADR (decision)

**Standard research:**
- HYPOTHESIS-DOC â†’ EXPERIMENT-LOG â†’ ADR

**Comprehensive research:**
- ALTERNATIVE-EXPLORATION â†’ HYPOTHESIS-DOC â†’ EXPERIMENT-LOG â†’ POC-DOC â†’ RESEARCH-FINDINGS â†’ ADR

**Zasada:** Start minimal, expand jeÅ›li potrzeba (don't overengineer).

---

### Q: Co jeÅ›li moja hipoteza zostaÅ‚a odrzucona?

**A: To SUCCESS, nie failure! Documented rejection jest valuable knowledge.**

**Workflow:**
1. Update HYPOTHESIS-DOC status â†’ `invalidated`
2. Complete EXPERIMENT-LOG z wnioskami (why rejected, lessons learned)
3. Optional: RESEARCH-FINDINGS (what we learned, alternative approaches)
4. Create ADR: "Decision NOT to pursue X (based on EXP-042)"

**Value:**
- Prevent future teams from repeating mistake
- Justify decision ("We considered X, tested, rejected because Y")
- Organizational learning (failures > successes dla knowledge)

---

### Q: Jak dÅ‚ugo trzymaÄ‡ research documents?

**A: Indefinitely (archive, don't delete).**

**Retention policy:**
- **Active research:** `/docs/research/` (current work)
- **Completed research:** `/docs/research/archive/YYYY/` (by year)
- **References:** ADR linkuje do archived research (traceability)

**Why retain:**
- Audit trail (compliance, especially regulated industries)
- Knowledge retention (new team members learn from past research)
- Prevent repetition ("We tried this in 2023, see archived EXP-015")

**Storage:**
- Git repository (version control, searchable)
- Minimal storage cost (Markdown files sÄ… small)

---

### Q: Czy Research Templates sÄ… TYLKO dla technical research?

**A: NIE! DziaÅ‚ajÄ… dla UX, business, product research teÅ¼.**

**Examples:**

**UX Research:**
- SPIKE-SOLUTION: Usability testing (5 users, 3 dni)
- EXPERIMENT-LOG: A/B test tracking (2 tygodnie)
- RESEARCH-FINDINGS: Quarterly UX insights

**Business Research:**
- HYPOTHESIS-DOC: "Pricing model X increases conversion 20%"
- EXPERIMENT-LOG: A/B test (pricing variants, 4 weeks)
- ALTERNATIVE-EXPLORATION: Pricing models comparison

**Product Research:**
- SPIKE-SOLUTION: Feature exploration (customer interviews, 5 dni)
- POC-DOC: MVP prototype (validate product-market fit)

**Zasada:** JeÅ›li formuÅ‚ujesz pytanie + potrzebujesz empirycznej odpowiedzi â†’ Research Templates apply!

---

### Q: Jak zmierzyÄ‡ ROI Research Templates adoption?

**A: Track metrics przed i po adoption.**

**Metrics:**

**M1: Knowledge Retention**
- Before: "Can you find results z past experiment X?" â†’ 30% success rate
- After: â†’ 90% success rate (documented, searchable)

**M2: Decision Quality**
- Before: X% decisions backed by data
- After: Y% decisions backed by documented research (target: >70%)

**M3: Time to Decision**
- Before: Average 8 tygodni (hypothesis â†’ decision)
- After: Average 4-6 tygodni (structured process, timeboxing)

**M4: Avoided Bad Decisions**
- Count: PoC with "STOP" recommendation â†’ prevented waste
- Estimate: $X saved by NOT pursuing failed PoC

**M5: Team Confidence**
- Survey: "How confident are you w research-backed decisions?" (1-10 scale)
- Target: Average >8/10

---

## Zasoby

### Templates

- [HYPOTHESIS-DOC.md](./HYPOTHESIS-DOC.md)
- [EXPERIMENT-LOG.md](./EXPERIMENT-LOG.md)
- [POC-DOC.md](./POC-DOC.md)
- [SPIKE-SOLUTION.md](./SPIKE-SOLUTION.md)
- [RESEARCH-FINDINGS.md](./RESEARCH-FINDINGS.md)
- [ALTERNATIVE-EXPLORATION.md](./ALTERNATIVE-EXPLORATION.md)
- [CONCEPT-BRANCH.md](./CONCEPT-BRANCH.md)

### PrzykÅ‚ady

Wszystkie przykÅ‚ady w katalogu: [`/examples/research/`](../../examples/research/)

**End-to-end workflow:**
1. [HYPOTHESIS-DOC-001-migracja-postgres-mongodb.md](../../examples/research/HYPOTHESIS-DOC-001-migracja-postgres-mongodb.md)
2. [EXPERIMENT-LOG-001-mongodb-benchmark.md](../../examples/research/EXPERIMENT-LOG-001-mongodb-benchmark.md)
3. [POC-DOC-001-mongodb-pilot-deployment.md](../../examples/research/POC-DOC-001-mongodb-pilot-deployment.md)
4. [RESEARCH-FINDINGS-001-performance-optimization-q1-2026.md](../../examples/research/RESEARCH-FINDINGS-001-performance-optimization-q1-2026.md)

**Standalone:**
- [SPIKE-SOLUTION-001-rust-wasm-rendering.md](../../examples/research/SPIKE-SOLUTION-001-rust-wasm-rendering.md)
- [ALTERNATIVE-EXPLORATION-001-baza-dokumentow.md](../../examples/research/ALTERNATIVE-EXPLORATION-001-baza-dokumentow.md)
- [CONCEPT-BRANCH-001-realtime-collab-websocket.md](../../examples/research/CONCEPT-BRANCH-001-realtime-collab-websocket.md)

### Dokumentacja ÅºrÃ³dÅ‚owa

- [PROPOZYCJA-1-Research-Branch-Templates.md](../../proposals/PROPOZYCJA-1-Research-Branch-Templates.md) - PeÅ‚na specyfikacja research templates

---

## Kontakt i Wsparcie

**Pytania?** UtwÃ³rz issue w projekcie lub skontaktuj siÄ™ z zespoÅ‚em dokumentacji.

**Feedback?** Sugestie dotyczÄ…ce templates sÄ… mile widziane - Research Templates to living framework!

**Contributions?** Pull requests z improvements/examples przyjmowane z wdziÄ™cznoÅ›ciÄ….

---

**Ostatnia aktualizacja:** 2025-12-29
**Wersja README:** 1.0
**Maintainer:** ZespÃ³Å‚ Dokumentacji Ishkarim
